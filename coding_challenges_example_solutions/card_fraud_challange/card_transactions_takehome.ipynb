{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Science Challenge: Card Transactions\n",
    "\n",
    "\n",
    "#### Question 1: Load\n",
    "\n",
    "-\t1.1 Programmatically load the data into your favorite analytical tool. \n",
    "-\t1.2 Please describe the structure of the data. Number of records and fields in each record?\n",
    "-\t1.3 Please provide some additional basic summary statistics for each field. Be sure to include a count of null, minimum, maximum, and unique values where appropriate.\n",
    "\n",
    "\n",
    "#### Question 2: Plot\n",
    "\n",
    "-\t2.1 Plot a histogram of the processed amounts of each transaction, the transactionAmount column. \n",
    "-   2.2 Report any structure you find and any hypotheses you have about that structure.\n",
    "\n",
    "\n",
    "#### Question 3: Data Wrangling - Duplicate Transactions\n",
    "You will notice a number of what look like duplicated transactions in the data set. One type of duplicated transaction is a reversed transaction, where a purchase is followed by a reversal. Another example is a multi-swipe, where a vendor accidentally charges a customer's card multiple times within a short time span.\n",
    "\n",
    "-\t3.1 Can you programmatically identify reversed and multi-swipe transactions?\n",
    "-\t3.2 What total number of transactions and total dollar amount do you estimate for the reversed transactions? For the multi-swipe transactions? (please consider the first transaction to be \"normal\" and exclude it from the number of transaction and dollar amount counts)\n",
    "-\t3.3 Did you find anything interesting about either kind of transaction?\n",
    "\n",
    "\n",
    "#### Question 4: Model\n",
    "\n",
    "Fraud is a problem for any bank. Fraud can take many forms, whether it is someone stealing a single credit card, to large batches of stolen credit card numbers being used on the web, or even a mass compromise of credit card numbers stolen from a merchant via tools like credit card skimming devices.\n",
    "-\t4.1 Each of the transactions in the dataset has a field called isFraud. Please build a predictive model to determine whether a given transaction will be fraudulent or not. Use as much of the data as you like (or all of it).\n",
    "-\t4.2 Provide an estimate of performance using an appropriate sample, and show your work.\n",
    "-\t4.3 Please explain your methodology (modeling algorithm/method used and why, what features/data you found useful, what questions you have, and what you would do next with more time)\n",
    "\n",
    "\n",
    "#### The following artifacts are valued:\n",
    "-\texplanations of your intent, methods, conclusions and any assumptions\n",
    "-\tclear, documented, and well-structured code\n",
    "-\tinstructions for running your code\n",
    "-\tmethods you attempted that didn't work\n",
    "-\tideas you didn't have time to complete but would have done with more time\n",
    "-\ta thorough write up with any pertinent visualizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to run this code:\n",
    "\n",
    "\n",
    "I) set up this environment:\n",
    "\n",
    "\n",
    "conda create -n xgboost_env -c conda-forge python=3 numpy pandas scikit-learn category_encoders ipython-notebook wget matplotlib=3 xgboost unzip \n",
    "\n",
    "source activate xgboost_env\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "II) launch the notebook:\n",
    "\n",
    "ipython notebook card_transactions_takehome.ipynb\n",
    "\n",
    "\n",
    "III) run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libs and check if versions are correct\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import category_encoders as ce\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "print(\"MacBook Air Early 2014; OS 10.14.6\")\n",
    "print(\"The Python version is %s.%s.%s\" % sys.version_info[:3])\n",
    "print('The numpy version is {}.'.format(np.__version__))\n",
    "print('The pandas version is {}.'.format(pd.__version__))\n",
    "print('The matplotlib version is {}.'.format(matplotlib.__version__))\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "print('The category_encoders version is {}.'.format(ce.__version__))\n",
    "print('The xgboost version is {}.'.format(xgboost.__version__))\n",
    "\n",
    "#The Python version is 3.5.5\n",
    "#The numpy version is 1.15.2.\n",
    "#The pandas version is 0.23.4.\n",
    "#The matplotlib version is 3.0.0.\n",
    "#The scikit-learn version is 0.20.0.\n",
    "#The category_encoders version is 2.1.0.\n",
    "#The xgboost version is 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to investigate df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to look at data frame shape, count NA per field\n",
    "def df_shape_nacount(df):\n",
    "    shape = df.shape \n",
    "    print('shape')\n",
    "    print(shape)\n",
    "    print('')\n",
    "\n",
    "    fields = list(df) \n",
    "    # count NAs \n",
    "    # treat inf as Na\n",
    "    pd.options.mode.use_inf_as_na = True\n",
    "    found_one = 0\n",
    "    for i in fields:  \n",
    "        if(sum(df[i].isna()))>0:\n",
    "            print (i)\n",
    "            print(\"Na count: \")\n",
    "            print (sum(df[i].isna()))  \n",
    "            print('')\n",
    "            found_one = 1\n",
    "    if(found_one ==0):\n",
    "        print('no NA found at all') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to look at data frame shape, count and list unique values per field\n",
    "def df_shape_unique_countlist(df):\n",
    "    shape = df.shape \n",
    "    print('shape')\n",
    "    print(shape)\n",
    "    print('')\n",
    "\n",
    "    fields = list(df) \n",
    "    # count and list unique values per field\n",
    "    for i in fields:  \n",
    "        print (i) \n",
    "        print ('unique count: ') \n",
    "        uniq = pd.unique(df[i])\n",
    "        print(uniq.shape[0])\n",
    "        print ('unique list') \n",
    "        print (uniq)\n",
    "        print ('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to look at data frame shape, count None per category field  \n",
    "def df_shape_nonecount(df):\n",
    "    shape = df.shape \n",
    "    print('shape')\n",
    "    print(shape)\n",
    "    print('')\n",
    "\n",
    "    fields = list(df) \n",
    "    # count and list ''  per field\n",
    "    for i in fields:\n",
    "        if(df[i].dtypes == 'O' and (sum(df[i] == '') > 0)):    \n",
    "            print (i) \n",
    "            print ('None count: ') \n",
    "            print (sum(df[i] == ''))\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model-related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to initialize and fit xgboost with a given 'scale_pos_weight_value'\n",
    "# this function also calls the accuracy calculation function \n",
    "# and the function to calculate the net $ result of applying the fitted model\n",
    "def trial_xgboost_scale_pos_weight(scale_pos_weight_value, x_var_train, x_var_eval, y_var_eval, transfeefrac):\n",
    "    # define model\n",
    "    model = XGBClassifier(scale_pos_weight=scale_pos_weight_value, seed=1)\n",
    "    # fit model\n",
    "    model.fit(X = x_var_train, y = label_train_encoded_y)\n",
    "    # predict\n",
    "    y_pred = model.predict(x_var_eval)\n",
    "    \n",
    "    print(\"scale_pos_weight used: \",  scale_pos_weight_value)\n",
    "    transamount = x_var_eval['transactionAmount']\n",
    "    # call function to calculate accuracy and cost\n",
    "    calc_accurcy(y_var_eval, y_pred)\n",
    "    # call function to calculate $ net\n",
    "    net = calc_cost(y_var_eval, y_pred, transamount, transfeefrac)\n",
    "    # return the fitted model and the $ net\n",
    "    return model, net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the accuracy values of a model \n",
    "# results are printed to screen\n",
    "def calc_accurcy(y, y_hat):\n",
    "    #count cases\n",
    "    sample_count = len(y)\n",
    "    fraud_count = sum(y)\n",
    "    fraud_pred_count = sum(y_hat)\n",
    "    \n",
    "    correct_fraud_pred_count = sum((y==1) & (y_hat==1))\n",
    "    correct_nofraud_pred_count = sum((y==0) & (y_hat==0))\n",
    "    incorrect_fraud_pred_count = sum((y==0) & (y_hat==1)) #false_fraud\n",
    "    incorrect_nofraud_pred_count = sum((y==1) & (y_hat==0)) #missed_fraud\n",
    "    print('count all frauds: ', fraud_count)\n",
    "    print('count correct fraud predictions: ', correct_fraud_pred_count)\n",
    "    print('count false fraud predictions: ', incorrect_fraud_pred_count)\n",
    "    print('count missed frauds: ', incorrect_nofraud_pred_count)\n",
    "\n",
    "    # calculate overall accuracy, precision, recall\n",
    "    # p is correct positive results divided by all positive results returned by the classifier. \n",
    "    p = correct_fraud_pred_count / fraud_pred_count\n",
    "    # r is correct positive results divided by all samples that should have been identified as positive. \n",
    "    r = correct_fraud_pred_count / fraud_count\n",
    "    # overall accuracy\n",
    "    o = (correct_fraud_pred_count + correct_nofraud_pred_count) /sample_count\n",
    "\n",
    "    # print overall accuracy, precision, recall w 5 d.p.\n",
    "    print('overall accuracy: ',round(o,3), 'precision: ', round(p,3), 'recall: ',round(r,3))\n",
    "    print('precision is the ability of the classifier not to label as positive a sample that is negative.')\n",
    "    print('The recall is the ability of the classifier to find all the positive samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the net $ result of applying the model\n",
    "def calc_cost(y,  y_hat, transamount, transfeefrac, benchmark = False):\n",
    "    if(benchmark):\n",
    "        # calculate w/o a fraud detection model:\n",
    "        # missed fraud array\n",
    "        missed_fraud = (y==1) # use all fraud\n",
    "        # cost is the sum of transamounts * missed_fraud cases (here = all fraud)\n",
    "        cost = round(sum(transamount*missed_fraud))\n",
    "        # income is the sum of transamounts * transfeefrac \n",
    "        income = round(sum(transamount*transfeefrac))    \n",
    "    else: \n",
    "        # calculate using the fraud model results provided:\n",
    "        # missed fraud array\n",
    "        missed_fraud = (y==1) & (y_hat==0)\n",
    "        # cost is the sum of transamounts * missed_fraud cases (zeros and ones)\n",
    "        cost = round(sum(transamount*missed_fraud))\n",
    "        # income is the sum of transamounts * transfeefrac * cases not flagged as fraud (zeros and ones)\n",
    "        income = round(sum(transamount*transfeefrac*(1-y_hat)))\n",
    "        \n",
    "    # $ net is income - cost\n",
    "    net = income - cost\n",
    "    #\n",
    "    print('---')\n",
    "    print('income $: ', income) \n",
    "    print('cost $: ', cost)\n",
    "    print('net $: ', net) \n",
    "    print('---')\n",
    "    # $ net is returned\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Load\n",
    "\n",
    "-\t1.1 Programmatically load the data into your favorite analytical tool the transactions data. \n",
    "-\t1.2 Please describe the structure of the data. Number of records and fields in each record?\n",
    "-\t1.3 Please provide some additional basic summary statistics for each field. Be sure to include a count of null, minimum, maximum, and unique values where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Programmatically load the data into  your favorite analytical tool the transactions data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip and rm what is no longer needed\n",
    "com = \"unzip transactions.txt.zip\"\n",
    "os.system(com)\n",
    "com = \"rm transactions.txt.zip\"\n",
    "os.system(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data into pd\n",
    "df = pd.read_json('transactions.txt', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Please describe the structure of the data. Number of records and fields in each record?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dimensions: 29 fields\n",
    "print('number of records, number of fields: ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserve the origional_index in a field called 'origional_index'\n",
    "df['origional_index'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 Please provide some additional basic summary statistics for each field. Be sure to include a count of null, minimum, maximum, and unique values where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a peek at the data\n",
    "df[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all fields as we cannot see them in the field above\n",
    "fields = list(df)\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NA values in all fields\n",
    "df_shape_nacount(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts of unique values per field and show values\n",
    "df_shape_unique_countlist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop fields that have only one value as not useful\n",
    "df.drop(['echoBuffer', 'merchantCity', 'merchantState', 'merchantZip', 'posOnPremises', 'recurringAuthInd'], axis='columns', inplace=True)\n",
    "# number of fields I am left with: \n",
    "df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check non numeric files for missing entries (=='') and count the missing entries’\n",
    "df_shape_nonecount(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I note that there are cases where the transactionType is not known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I do not comprehend the following fields and could not ask for clarification\n",
    "# I hence drop them from further analysis as using fields that I do not understand in transaction \n",
    "# modeling would be risky\n",
    "df.drop(['posConditionCode', 'posEntryMode'], axis='columns', inplace=True)\n",
    "# number of fields I am left with: \n",
    "df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if 'accountNumber' = 'customerId'\n",
    "df[df['accountNumber'] == df['customerId']].shape[0] == df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop customerId as it is a duplicate of accountNumber\n",
    "df.drop(['customerId'], axis='columns', inplace=True)\n",
    "# number of fields I am left with: \n",
    "df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describing fields as summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categroical fields\n",
    "df.describe(include='O') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bool fields\n",
    "df.describe(include='bool') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical fields\n",
    "df.describe(include=np.number) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Plot\n",
    "\n",
    "-\t2.1 Plot a histogram of the processed amounts of each transaction, the transactionAmount column. \n",
    "-   2.2 Report any structure you find and any hypotheses you have about that structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  2.1 Plot a histogram of the processed amounts of each transaction, the transactionAmount column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transactionAmount'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the distribution is dominated by small transactions and counter per bin decrease with transaction size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I separate fraud from nofraud and count the respective occurrences\n",
    "fraud = df[df['isFraud']==1]\n",
    "nofraud = df[df['isFraud']==0]\n",
    "\n",
    "print('fraud count: ', fraud.shape[0])\n",
    "print('nofraud count: ', nofraud.shape[0])\n",
    "print('all sample count: ', df.shape[0])\n",
    "print('Fraud is rare. Only  ', round(fraud.shape[0]/df.shape[0] * 100, 2), ' % of all samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraud is rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing the histogram of transaction amounts of the entire sample with ....\n",
    "nbins = 100\n",
    "xmin = 0\n",
    "xmax = 1000\n",
    "\n",
    "fig = plt.figure(); \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(df['transactionAmount'], bins=nbins,range=[xmin,xmax])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... the histogram of transaction amounts of fraud and with ...\n",
    "fig = plt.figure(); \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(fraud['transactionAmount'], bins=nbins,range=[xmin,xmax])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... the histogram of transaction amounts of nofraud \n",
    "fig = plt.figure(); \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(nofraud['transactionAmount'], bins=nbins,range=[xmin,xmax])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraud appears to have larger transaction amount than nofraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same comparison of transaction amount in all samples vs ...\n",
    "df['transactionAmount'].describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transaction amount of fraud samples vs ...\n",
    "fraud['transactionAmount'].describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transaction amount of no fraud samples \n",
    "nofraud['transactionAmount'].describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  2.2 Report any structure you find and any hypotheses you have about that structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: \n",
    "- The distribution of transactionAmount is dominated by small transactions and counter per bin decrease with transaction size\n",
    "- Fraud appears to have larger transaction amount (e.g. higher median)\n",
    "- Hypotheses: the transactionAmount itself could be a useful feature for predicting fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Data Wrangling - Duplicate Transactions\n",
    "You will notice a number of what look like duplicated transactions in the data set. One type of duplicated transaction is a reversed transaction, where a purchase is followed by a reversal. Another example is a multi-swipe, where a vendor accidentally charges a customer's card multiple times within a short time span.\n",
    "\n",
    "-\t3.1 Can you programmatically identify reversed and multi-swipe transactions?\n",
    "-\t3.2 What total number of transactions and total dollar amount do you estimate for the reversed transactions? For the multi-swipe transactions? (please consider the first transaction to be \"normal\" and exclude it from the number of transaction and dollar amount counts)\n",
    "-\t3.3 Did you find anything interesting about either kind of transaction?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out address verifications (assuming that there are no omission or commission errors in this field)\n",
    "df_subset = df[df['transactionType'] != 'ADDRESS_VERIFICATION']\n",
    "# number of samples I am left with: \n",
    "df_subset.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1a Can you programmatically identify reversed transactions?\n",
    "##### 3.2a What total number of transactions and total dollar amount do you estimate for the reversed transactions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# programmatically identify reversed:\n",
    "\n",
    "# reversed transactions can be easily identified the REVERSAL flag in the transactionType field\n",
    "# (assuming that there are no omission or commission errors of this field)\n",
    "df_reversed = df_subset[df_subset ['transactionType'] == 'REVERSAL']\n",
    "\n",
    "print('the number of reversed transaction is :', df_reversed.shape[0])\n",
    "print('the total transaction amount of the reversed transaction is $:', round(sum(df_reversed['transactionAmount'])))\n",
    "\n",
    "# I can link the result back to the df using the 'origional_index' field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of reversed transactions e.g. per merchantCategoryCode\n",
    "df_reversed['merchantCategoryCode'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online_retail followed by fastfood dominates merchantCategoryCode of the reversed transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1b Can you programmatically identify multi-swipe transactions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the reversed transactions from the subset df\n",
    "df_subset = df_subset[df_subset ['transactionType'] != 'REVERSAL']\n",
    "# number of samples I am left with: \n",
    "df_subset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am looking for multiple occurrences of the same 'event' (once ADDRESS_VERIFICATION and REVERSAL has been taken out)\n",
    "\n",
    "# assuming that an event must involve multiple occurrences of identical combinations of:\n",
    "# 'accountNumber', 'cardLast4Digits', 'cardCVV', 'merchantCategoryCode','merchantCountryCode', 'merchantName'\n",
    "# I do not use 'transactionAmount' as somebody may increase the amount for a repeat swipe\n",
    "df_possible_multi_swipe = df_subset[df_subset.duplicated(['accountNumber', 'cardLast4Digits', 'cardCVV','merchantCategoryCode', 'merchantCountryCode', 'merchantName'], keep = False)]\n",
    "del(df_subset)\n",
    "# number of suspectsamples:\n",
    "df_possible_multi_swipe.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am assuming no NA or missing values and no fake entries (a merchant can be identified as opposed to appear as somebody else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I translate transactionDateTime into a pd datetime64 value and map it to a new field called: date_time'\n",
    "transactionDateTime = df_possible_multi_swipe['transactionDateTime'].astype(\"datetime64\")\n",
    "df_possible_multi_swipe.insert(loc = df_possible_multi_swipe.shape[1], column = 'date_time', value = transactionDateTime)\n",
    "del(transactionDateTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure that the df is sorted I sort the suspect sample by  the following fields: \n",
    "# 'accountNumber', 'cardLast4Digits', 'cardCVV','merchantCategoryCode', 'merchantCountryCode', 'merchantName', 'date_time'\n",
    "df_possible_multi_swipe = df_possible_multi_swipe.sort_values(by=['accountNumber', 'cardLast4Digits', 'cardCVV','merchantCategoryCode', 'merchantCountryCode', 'merchantName', 'date_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume that multi swipe can only occur when card is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming that multi swipe can only occur when card is present\n",
    "df_possible_multi_swipe = df_possible_multi_swipe[df_possible_multi_swipe['cardPresent']==True]\n",
    "# number of suspectsamples:\n",
    "# counts dropped to ~1/3\n",
    "df_possible_multi_swipe.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I calculate the time difference between subsequent transactions in seconds \n",
    "delta_subsequent_enties_in_sec = (df_possible_multi_swipe['date_time']-df_possible_multi_swipe['date_time'].shift())/ np.timedelta64(1, 's')\n",
    "# set the first entry to zero\n",
    "delta_subsequent_enties_in_sec[0] = 0 \n",
    "\n",
    "# I reset the indices of df_possible_multi_swipe and delta_subsequent_enties_in_sec so they \n",
    "# I can insert delta_subsequent_enties_in_sec values\n",
    "df_possible_multi_swipe.index = pd.RangeIndex(len(df_possible_multi_swipe.index))\n",
    "delta_subsequent_enties_in_sec.index = pd.RangeIndex(len(delta_subsequent_enties_in_sec.index))\n",
    "\n",
    "# I map the time difference between subsequent transactions in seconds to a field called: 'delta'\n",
    "df_possible_multi_swipe.insert(loc = df_possible_multi_swipe.shape[1], column = 'delta', value = delta_subsequent_enties_in_sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define multi-swipe transactions as occurrences of identical combinations of: 'accountNumber', 'cardLast4Digits', 'cardCVV', 'merchantCategoryCode','merchantCountryCode', 'merchantName' within and 3 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dif time delta > 0 and < 180 sec\n",
    "mindif = 0\n",
    "maxdif = 180 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset > mindif and < maxdif\n",
    "multi_swipe = df_possible_multi_swipe[(df_possible_multi_swipe['delta'] > mindif) & (df_possible_multi_swipe['delta'] < maxdif)]\n",
    "del(df_possible_multi_swipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2b What total number of transactions and total dollar amount do you estimate for the multi-swipe transactions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of multi-swipe transactions found: ', multi_swipe.shape[0])\n",
    "# I can link the back to the df using the 'origional_index' field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total dollar amount due to multi-swipe transactions: ', sum(multi_swipe['transactionAmount']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 Did you find anything interesting about either kind of transaction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the fraction of multi-swipes flagged as fraud is: ', round(sum(multi_swipe['isFraud'])/multi_swipe.shape[0]*100,2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fraction of multi-swipes flagged as fraud did not appear unusually high...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now comparing normalized histograms of transactionAmount for:\n",
    "# the entire sample, fraud, nofraud, reversed transactions, and the multi swipe transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all transactions\n",
    "fig = plt.figure(); \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(df['transactionAmount'], density = True, bins=nbins,range=[xmin,xmax])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lots of very small $ amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraud\n",
    "fig = plt.figure(); \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(fraud['transactionAmount'], density = True, bins=nbins,range=[xmin,xmax])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fraud peak at $100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nofraud \n",
    "fig = plt.figure(); \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(nofraud['transactionAmount'], density = True, bins=nbins,range=[xmin,xmax])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nofraud similar to all samples (as expected given that fraud is rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reversed:  lots of very small amounts similar to all samples\n",
    "fig = plt.figure(); \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(df_reversed['transactionAmount'], density = True, bins=nbins,range=[xmin,xmax])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reversed has lots of very small amounts similar to all sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_swipe has a second peak at ~$50 which is different from all samples and non-fraud\n",
    "fig = plt.figure(); \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(multi_swipe['transactionAmount'], density = True, bins=nbins,range=[xmin,xmax])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi_swipe has a second peak at ~ $50 which is different from all sample and non-fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of counting e.g. per merchantCategoryCode\n",
    "multi_swipe['merchantCategoryCode'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now FAST FOOD dominates (previously it was online_retail); however the assumption that multi swipes can only occur when a card is present is risky as an online retail store can charge a card multiple times also...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(multi_swipe)\n",
    "del(fraud)\n",
    "del(nofraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Model\n",
    "\n",
    "Fraud is a problem for any bank. Fraud can take many forms, whether it is someone stealing a single credit card, to large batches of stolen credit card numbers being used on the web, or even a mass compromise of credit card numbers stolen from a merchant via tools like credit card skimming devices.\n",
    "-\t4.1 Each of the transactions in the dataset has a field called isFraud. Please build a predictive model to determine whether a given transaction will be fraudulent or not. Use as much of the data as you like (or all of it).\n",
    "-\t4.2 Provide an estimate of performance using an appropriate sample, and show your work.\n",
    "-\t4.3 Please explain your methodology (modeling algorithm/method used and why, what features/data you found useful, what questions you have, and what you would do next with more time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Each of the transactions in the dataset has a field called isFraud. Please build a predictive model to determine whether a given transaction will be fraudulent or not. Use as much of the data as you like (or all of it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by transactionType ['PURCHASE' 'ADDRESS_VERIFICATION' 'REVERSAL' ''] and call the result: df_subset_fraud\n",
    "\n",
    "# drop 'ADDRESS_VERIFICATION'\n",
    "df_subset_fraud = df[df['transactionType'] != 'ADDRESS_VERIFICATION']\n",
    "# drop 'REVERSAL'\n",
    "df_subset_fraud = df_subset_fraud[df_subset_fraud ['transactionType'] != 'REVERSAL']\n",
    "\n",
    "print('initial number of sample , number fields: ', df.shape)\n",
    "print('number of sample , number fields left: ', df_subset_fraud.shape)\n",
    "print('number of sample , number fields left: ', df_subset_fraud.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list fields \n",
    "fields = list(df_subset_fraud)\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I drop a number of fields either because they may lead to overfitting (e.g. origional_index), \n",
    "# or they should not be a fraud predictor (e.g. cardLast4Digits),\n",
    "# or because this simplifies my proof of concept model (e.g. enteredCVV or dateOfLastAddressChange), \n",
    "# or because there are too many categories for on hot encoding (e.g. merchantName)\n",
    "df_subset_fraud.drop(['accountNumber', 'accountOpenDate', 'acqCountry', 'cardLast4Digits', 'cardCVV', 'currentExpDate', 'dateOfLastAddressChange', 'enteredCVV', 'merchantName','transactionDateTime', 'transactionType', 'origional_index'], axis='columns', inplace=True)\n",
    "print('number fields left: ', df_subset_fraud.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dropped a number of fields either because they may lead to overfitting, or they should not be a fraud predictor, or because this simplifies my proof of concept model,  or because there are too many categories for on hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list fields \n",
    "fields = list(df_subset_fraud)\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts of unique values per field and show values\n",
    "df_shape_unique_countlist(df_subset_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check non numeric files for missing entries (=='') and count the missing entries’\n",
    "df_shape_nonecount(df_subset_fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only merchantCountryCode has '' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features for one hot encoding\n",
    "df_fro_ohe = df_subset_fraud[['merchantCategoryCode', 'merchantCountryCode']]\n",
    "print('number of sample , number fields : ', df_fro_ohe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop those features from df_subset_fraud\n",
    "df_subset_fraud.drop(['merchantCategoryCode', 'merchantCountryCode'], axis='columns', inplace=True)\n",
    "print('number of sample , number fields left: ', df_subset_fraud.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the one hot encoding of the non-ordinal category variables\n",
    "ohe = ce.OneHotEncoder(handle_unknown='ignore', use_cat_names=True)\n",
    "x_train_ohe = ohe.fit_transform(df_fro_ohe)\n",
    "del(ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat df_subset_fraud with the x_train_ohe (the one hot encoding)\n",
    "df_subset_fraud = pd.concat([df_subset_fraud, x_train_ohe], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dims and lsit the fields \n",
    "print('number of sample , number fields : ', df_subset_fraud.shape)\n",
    "fields = list(df_subset_fraud)\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a peek at the data for modeling \n",
    "df_subset_fraud[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I split the data into 3 sets: a training set, a test set and a holdout set. \n",
    "# the purpose of the 3 sets is for training, hyperparameter tuning and final testing of the model, respectively\n",
    "# the training set is 50% of the data and the test and holdout sets are 25% respectively\n",
    "\n",
    "np.random.seed(1)\n",
    "seed = 1\n",
    "test_size = 0.5\n",
    "\n",
    "# define the Y variable\n",
    "Y = df_subset_fraud.iloc[:, 5]\n",
    "\n",
    "# define the fieldscolumes for the X variables\n",
    "li_1 =[0,1,2,3,4]\n",
    "li_2 = list(range(6, df_subset_fraud.shape[1]))\n",
    "joinedlist = li_1 + li_2\n",
    "X = df_subset_fraud.iloc[:,joinedlist]\n",
    "\n",
    "# break the dataset into set into a training and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "# break the test set into a test and holdout set\n",
    "x_test, x_holdout, y_test, y_holdout = train_test_split(x_test, y_test, test_size=test_size, random_state=seed)\n",
    "\n",
    "# print the number of samples and fields in the various subsets \n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_holdout.shape)\n",
    "print(y_holdout.shape)\n",
    "\n",
    "del(df_subset_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the fraud variable for XGBClassifier\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(y_train)\n",
    "label_train_encoded_y = label_encoder.transform(y_train)\n",
    "\n",
    "del(label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2a Provide an estimate of performance using an appropriate sample, and show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### How to eval a fraud classifier (a model that can block fraud)? \n",
    "\n",
    "###### By considering net $ profit: \n",
    "calculating....\n",
    "\n",
    "- 'income' as all transactions that were not blocked * 0.015 (assuming a 1.5% fee on transactions)\n",
    "- 'cost' as the total value of fraud transactions that were not detected by the model\n",
    "- 'net' as income - cost. \n",
    "\n",
    "\n",
    "$ Net is the amount we want to maximize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### benchmark $ net when doing nothing... I do not try to block any fraud... on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark when doing nothing... I do not try to block any fraud... on the test data\n",
    "transamount = x_test['transactionAmount']\n",
    "transfeefrac = 0.015\n",
    "y_no_model = ''\n",
    "calc_cost(y_test, y_no_model, transamount, transfeefrac, benchmark = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $393,062 would be earned if there was no fraud\n",
    "\n",
    "\n",
    "But $660,087 had to be paid b/c we did not have a model in place to reject fraud\n",
    "\n",
    "So we end up with a net loss of $267,025\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Given that the fraud is rare we put more weight on fraud cases when training the xgboost model (or any model)\n",
    "\n",
    "\n",
    "We incrementally increase the weight on fraud cases while keeping track of the $ net result of applying the model to the test data.\n",
    "\n",
    "\n",
    "#### We keep the model with the largest $ net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfeefrac = 0.015\n",
    "\n",
    "# loop over increasing 'scale_pos_weight' values\n",
    "scale_pos_weight_list = [1,2,4,8,16,32,64,128]\n",
    "for current_scale_pos_weight in scale_pos_weight_list:\n",
    "    # fit the model with the current 'scale_pos_weight' value\n",
    "    model, net = trial_xgboost_scale_pos_weight(current_scale_pos_weight, x_train, x_test, y_test, transfeefrac)\n",
    "    #\n",
    "    if(current_scale_pos_weight==1):\n",
    "        best_model = model\n",
    "        best_scale_pos_weight = current_scale_pos_weight\n",
    "        best_net = net\n",
    "    # compare if the current net is > than the highest one so far. \n",
    "    # if yes update the best model and net\n",
    "    if(net > best_net):\n",
    "        best_model = model\n",
    "        best_scale_pos_weight = current_scale_pos_weight\n",
    "        best_net = net\n",
    "    #\n",
    "print('the maximum net of $: ', round(best_net), ' was archived with a scale_pos_weight of: ', best_scale_pos_weight)\n",
    "model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model for the best model in terms of overall accuracy, precision, and recall is not very strong in terms of both precision but also recall. BUT it does earn the most $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2b Provide an estimate of performance using an appropriate sample, and show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I run my chosen model on the holdout sample (that was not involved in model fitting or hyper parameter tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### benchmark $ net when doing nothing... I do not try to block any fraud... on the holdout sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark $ net when doing nothing... I do not try to block any fraud... on the holdout sample\n",
    "# loss to of all fraud:\n",
    "\n",
    "transamount = x_holdout['transactionAmount']\n",
    "transfeefrac = 0.015\n",
    "y_no_model = ''\n",
    "calc_cost(y_holdout, y_no_model, transamount, transfeefrac, benchmark = True)\n",
    "\n",
    "####\n",
    "# so $393407 would be earned if there was no fraud\n",
    "# but $674133 had to be paid b/c we did not have a model in place to reject fraud\n",
    "# so we end up with a net loss of $280726"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### applying the model to the holdout sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the model to the holdout sample\n",
    "y_pred = model.predict(x_holdout)\n",
    "transamount = x_holdout['transactionAmount']\n",
    "transfeefrac = 0.015\n",
    "calc_accurcy(y_holdout, y_pred)\n",
    "net = calc_cost(y_holdout, y_pred, transamount, transfeefrac)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  the net when applying the model to the holdout sample is comparable to the net I got for the test sample  \n",
    "\n",
    "### As opposed to a net loss of USD -280,726 the model results in a net profit (> USD 50,000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "\n",
    "#importance_types\n",
    "#‘weight’ - the number of times a feature is used to split the data across all trees.\n",
    "#‘gain’ - the average gain across all splits the feature is used in.\n",
    "#‘cover’ - the average coverage across all splits the feature is used in.\n",
    "#‘total_gain’ - the total gain across all splits the feature is used in.\n",
    "#‘total_cover’ - the total coverage across all splits the feature is used in.\n",
    "\n",
    "# I use 'total_gain'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_importance(model, importance_type='total_gain', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print feature importance in descending order of 'total_gain'\n",
    "total_gain = model.get_booster().get_score(importance_type = 'total_gain')\n",
    "total_gain_sorted = sorted(total_gain.items(), key=lambda x: x[1], reverse=True)\n",
    "total_gain_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could now decide to e.g. to only use features with total_gain > 10k and refit the model with the subset of features that remains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 Please explain your methodology (modeling algorithm/method used and why, what features/data you found useful, what questions you have, and what you would do next with more time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modeling algorithm/method used and why:\n",
    "- I used XGBoost as it is the top algorithm when high accuracy is the priority\n",
    "- Given that fraud is rare I tested model versions with 'increased weight of the minority class'\n",
    "- I selected the best model version based on maximum net profit \n",
    "\n",
    "##### Features/data I found useful\n",
    "- The most important features were the transactionAmount , whether the card was present and merchantCategoryCode_fuel. \n",
    "- The creditLimit, currentBalance and various other merchantCategoryCodes were also important features. \n",
    "\n",
    "##### Questions I would have include:\n",
    "-  e.g. \"how are the isfraud labels derived and how reliable are they\", \"are multi swipes typically flagged as fraud?\"...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I also tried (but it did not work) to: \n",
    "\n",
    "- include the multi swipe result in as a feature in the fraud prediction model\n",
    "- include the delta between the transaction day and the day of last address change \n",
    "- include a feature comparing the card CVV and the entered CVV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What I  would do next with more time:\n",
    "- Look into what fraction of multi swipes were reversed \n",
    "- Look into the relationship between multi swipes, reversals and fraud transactions\n",
    "\n",
    "- I could trial comprehensive hyper parameter tuning\n",
    "- I could refine the net profit model assumptions\n",
    "- I could engineer features e.g. feature crosses such as transactionAmount with merchant category \n",
    "- I could engineer features e.g.  available funds / credit limit \n",
    "- I could engineer features e.g. time of day, day of week, week of year\n",
    "\n",
    "\n",
    "- I could try a deep autoencoder network model to detect anomalies\n",
    "- I could try a deep learning classifier with a very large sample and many feature crosses\n",
    "\n",
    "\n",
    "- I could refine the net profit model by refining the costs of fraud and intervention for different transactionAmount bins and prioritizing which transactionAmount bins to focus the detection modeling effort on. \n",
    "\n",
    "\n",
    "- I could engineer cardnumber-specific features such as:\n",
    "- typical max and mean amount of individual transactionAmount, transactionAmounts per day, transactionAmounts  per week, transactionAmounts per month and the delta of those from the transaction in question\n",
    "- as above but crossed with e.g. merchant category and perhaps hour of day or day of week\n",
    "\n",
    "- changes in purchase platform (in person, desktop, mobile) within a short time\n",
    "- changes in purchase location (large distances between locations) within a short time interval\n",
    "- a cross of the purchase platform and purchase location change within a short time\n",
    "\n",
    "\n",
    "- occurrence of multiple online purchases within a short amount of time\n",
    "- as above by binned by transactionAmounts to catch many small amounts\n",
    "\n",
    "- WRITE TESTS\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
